iFirst of all, don't forget to remove the input files of
the previous assignments:

    hdfs dfs -rm /user/cloudera/input/*

Then, in order to generate the datasets use the following commands:

    python make_join2data.py y 1000 13 > join2_gennumA.txt
    python make_join2data.py y 2000 17 > join2_gennumB.txt
    python make_join2data.py y 3000 19 > join2_gennumC.txt
    python make_join2data.py n 100  23 > join2_genchanA.txt
    python make_join2data.py n 200  19 > join2_genchanB.txt
    python make_join2data.py n 300  37 > join2_genchanC.txt

Don't forget to put the various data files on HDFS:

    hdfs dfs -put ...


In order to run the Hadoop command, remember to use the full path
of your Python scripts, both for -mapper and -reducer. You might wish
to use the $PWD environment for that purpose:

    cd week4
    hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
       -input /user/cloudera/input \
       -output /user/cloudera/output_join \
       -mapper "$PWD"/join1_mapper.py \
       -reducer "$PWD"/join1_reducer.py

If you need to re-run that command, don't forget to remove the previous
output files:

    hdfs dfs -rm -r -f /user/cloudera/output_join

